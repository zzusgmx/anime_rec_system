# recommendation/management/commands/preprocess_data.py

from django.core.management.base import BaseCommand
from django.utils import timezone
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import logging

logger = logging.getLogger('django')


class PreprocessDataPipeline:
    """é‡å­é¢„å¤„ç†ç®¡é“ - ä¼˜åŒ–æ¨èç³»ç»Ÿè¾“å…¥çŸ©é˜µ"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=0.95)  # ä¿ç•™95%çš„æ–¹å·®

    def run(self):
        logger.info("ğŸ§  å¯åŠ¨é‡å­é¢„å¤„ç†ç®¡é“...")
        # 1. ç‰¹å¾æå–
        raw_features = self._extract_features()

        # 2. æ•°æ®æ ‡å‡†åŒ–
        normalized_features = self._normalize_data(raw_features)

        # 3. ç»´åº¦çº¦ç®€
        reduced_features = self._reduce_dimensions(normalized_features)

        # 4. ç‰¹å¾å‘é‡åŒ–
        vectorized_features = self._vectorize_features(reduced_features)

        logger.info(
            f"âœ¨ é‡å­é¢„å¤„ç†ç®¡é“å®Œæˆ: {vectorized_features.shape[0]}è¡ŒÃ—{vectorized_features.shape[1]}åˆ—çš„ç‰¹å¾çŸ©é˜µ")
        return vectorized_features

    def _extract_features(self):
        """ä»æ•°æ®åº“æå–åŸå§‹ç‰¹å¾"""
        from anime.models import Anime
        from recommendation.models import UserRating, UserComment, UserFavorite

        logger.info("ğŸ” æ‰§è¡Œç‰¹å¾æå–...")
        # è¿™é‡Œå®ç°ä½ çš„ç‰¹å¾æå–é€»è¾‘
        # ä¾‹å¦‚ï¼Œä»æ¨¡å‹ä¸­æå–å„ç§ç‰¹å¾å¹¶ç»„ç»‡æˆDataFrame

        # ç¤ºä¾‹ä»£ç 
        anime_features = pd.DataFrame(
            list(Anime.objects.values('id', 'rating_avg', 'rating_count',
                                      'view_count', 'favorite_count', 'popularity'))
        )

        return anime_features

    def _normalize_data(self, features_df):
        """å°†æ•°æ®æ ‡å‡†åŒ–åˆ°ç›¸åŒé‡çº§"""
        logger.info("ğŸ“Š æ‰§è¡Œæ•°æ®æ ‡å‡†åŒ–...")

        # åˆ›å»ºç‰¹å¾çŸ©é˜µå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df = features_df.copy()

        # é€‰æ‹©éœ€è¦æ ‡å‡†åŒ–çš„æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
        numeric_cols = [col for col in numeric_cols if col != 'id']

        # åº”ç”¨æ ‡å‡†åŒ–å˜æ¢
        if numeric_cols:
            df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])

        return df

    def _reduce_dimensions(self, normalized_df):
        """ä½¿ç”¨PCAæ‰§è¡Œç»´åº¦çº¦ç®€"""
        logger.info("ğŸ§© æ‰§è¡Œç»´åº¦çº¦ç®€...")

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df = normalized_df.copy()

        # ä¿å­˜IDåˆ—
        ids = df['id'].values if 'id' in df.columns else None

        # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œé™ç»´
        numeric_cols = df.select_dtypes(include=['float64']).columns
        numeric_cols = [col for col in numeric_cols if col != 'id']

        if len(numeric_cols) > 2 and len(df) > 10:  # è‡³å°‘éœ€è¦3ä¸ªç‰¹å¾å’Œè¶³å¤Ÿçš„æ ·æœ¬é‡
            # åº”ç”¨PCA
            reduced_data = self.pca.fit_transform(df[numeric_cols])

            # åˆ›å»ºæ–°çš„DataFrame
            component_cols = [f'pc_{i + 1}' for i in range(reduced_data.shape[1])]
            reduced_df = pd.DataFrame(reduced_data, columns=component_cols)

            # æ·»åŠ å›IDåˆ—
            if ids is not None:
                reduced_df['id'] = ids

            logger.info(f"âœ‚ï¸ ç»´åº¦ä»{len(numeric_cols)}é™åˆ°{reduced_data.shape[1]}")
            return reduced_df
        else:
            logger.info("âš ï¸ ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡é™ç»´")
            return df

    def _vectorize_features(self, features_df):
        """å°†ç‰¹å¾è½¬æ¢ä¸ºå‘é‡å½¢å¼ï¼Œé€‚åˆæœºå™¨å­¦ä¹ æ¨¡å‹ä½¿ç”¨"""
        logger.info("ğŸ§® æ‰§è¡Œç‰¹å¾å‘é‡åŒ–...")

        # å®ç°ç‰¹å¾å‘é‡åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼Œå¤„ç†ç±»åˆ«ç‰¹å¾ã€æ–‡æœ¬ç‰¹å¾ç­‰

        # åœ¨è¿™ä¸ªç®€å•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åªè¿”å›å·²ç»å¤„ç†è¿‡çš„DataFrame
        return features_df


class Command(BaseCommand):
    help = 'æ‰§è¡Œæ•°æ®é¢„å¤„ç†æµæ°´çº¿ï¼Œä¼˜åŒ–æ¨èç³»ç»Ÿçš„è¾“å…¥çŸ©é˜µ'

    def add_arguments(self, parser):
        parser.add_argument('--output', type=str, help='è¾“å‡ºå¤„ç†åç‰¹å¾çŸ©é˜µçš„æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')

    def handle(self, *args, **options):
        if options['debug']:
            logging.basicConfig(level=logging.DEBUG)

        self.stdout.write(self.style.SUCCESS('ğŸš€ å¯åŠ¨é‡å­é¢„å¤„ç†ç®¡é“'))

        try:
            # å®ä¾‹åŒ–å¹¶è¿è¡Œé¢„å¤„ç†ç®¡é“
            pipeline = PreprocessDataPipeline()
            processed_data = pipeline.run()

            # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜ç»“æœ
            if options['output']:
                processed_data.to_csv(options['output'], index=False)
                self.stdout.write(self.style.SUCCESS(
                    f'ğŸ’¾ ç‰¹å¾çŸ©é˜µå·²ä¿å­˜è‡³: {options["output"]}'
                ))

            self.stdout.write(self.style.SUCCESS(
                f'âœ… é¢„å¤„ç†å®Œæˆ: ç”Ÿæˆ{processed_data.shape[0]}Ã—{processed_data.shape[1]}ç‰¹å¾çŸ©é˜µ'
            ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f'âŒ é¢„å¤„ç†å¤±è´¥: {str(e)}'))
            import traceback
            self.stdout.write(self.style.ERROR(traceback.format_exc()))